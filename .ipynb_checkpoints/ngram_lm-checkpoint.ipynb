{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 3: n-gram LM\n",
    "----\n",
    "\n",
    "Due date: October 4th, 2023\n",
    "\n",
    "Points: 105\n",
    "\n",
    "Goals:\n",
    "- understand the difficulties of counting and probablities in NLP applications\n",
    "- work with real world data to build a functioning language model\n",
    "- stress test your model (to some extent)\n",
    "\n",
    "Complete in groups of: __one (individually)__\n",
    "\n",
    "Allowed python modules:\n",
    "- `numpy`, `matplotlib`, and all built-in python libraries (e.g. `math` and `string`)\n",
    "- do not use `nltk` or `pandas`\n",
    "\n",
    "Instructions:\n",
    "- Complete outlined problems in this notebook. \n",
    "- When you have finished, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__. \n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
    "- Double check that you have completed Task 0.\n",
    "- Submit your work on Gradescope.\n",
    "- Double check that your submission on Gradescope looks like you believe it should __and__ that all partners are included (for partner work).\n",
    "\n",
    "6120 students: complete __all__ problems.\n",
    "\n",
    "4120 students: you are not required to complete problems marked \"CS 6120 REQUIRED\". If you complete these you will not get extra credit. We will not take points off if you attempt these problems and do not succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 0: Name, References, Reflection (5 points)\n",
    "---\n",
    "\n",
    "Name: Harishraj Udaya Bhaskar \n",
    "\n",
    "References\n",
    "---\n",
    "List the resources you consulted to complete this homework here. Write one sentence per resource about what it provided to you. If you consulted no references to complete your assignment, write a brief sentence stating that this is the case and why it was the case for you.\n",
    "\n",
    "(Example)\n",
    "- https://docs.python.org/3/tutorial/datastructures.html\n",
    "    - Read about the the basics and syntax for data structures in python.\n",
    "    \n",
    "- https://ramblersm.medium.com/the-significance-of-perplexity-in-evaluating-llms-and-generative-ai-62e290e791bc#:~:text=Thus%20mathematically%2C%20the%20formula%20to,w_%7Bi%2D1%7D).\n",
    "    - Got a understanding of Perplexity from this source\n",
    "\n",
    "AI Collaboration\n",
    "---\n",
    "Following the *AI Collaboration Policy* in the syllabus, please cite any LLMs that you used here and briefly describe what you used them for. Additionally, provide comments in-line identifying the specific sections that you used LLMs on, if you used them towards the generation of any of your answers.\n",
    "\n",
    "Reflection\n",
    "----\n",
    "Answer the following questions __after__ you complete this assignment (no more than 1 sentence per question required, this section is graded on completion):\n",
    "\n",
    "1. Does this work reflect your best effort?\n",
    "Yes, it does reflect my best work\n",
    "2. What was/were the most challenging part(s) of the assignment?\n",
    "The generation part was extremely challenging and completely new for me \n",
    "3. If you want feedback, what function(s) or problem(s) would you like feedback on and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Berp Data Write-Up (5 points)\n",
    "---\n",
    "\n",
    "Every time you use a data set in an NLP application (or in any software application), you should be able to answer a set of questions about that data. Answer these now. Default to no more than 1 sentence per question needed. If more explanation is necessary, do give it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the __berp__ data set.\n",
    "\n",
    "1. Where did you get the data from? https://www1.icsi.berkeley.edu/Speech/berp.html\n",
    "2. How was the data collected (where did the people acquiring the data get it from and how)?  The data was collected by ICSI(International Computer Science Institute)\n",
    "3. How large is the dataset? (# lines, # tokens) The datasets contains 7500 sentences, with 1500 words, comprising 6.4 hours of speech\n",
    "4. What is your data? (i.e. newswire, tweets, books, blogs, etc) The datsets is a database of restaurants in the berkely area \n",
    "5. Who produced the data? (who were the authors of the text? Your answer might be a specific person or a particular group of people) The data was collected by ICSI(International Computer Science Institute) which was headed by Nelson Morgan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Implement an n-gram Language Model (90 points)\n",
    "----\n",
    "\n",
    "Implement the `LanguageModel` class as outlined in the provided `lm_starter.py` file. Do not change function signatures (the unit tests that we provide and in the autograder will break).\n",
    "\n",
    "Your language model:\n",
    "- *must* work for both the unigram and bigram cases (5 points are allocated to an experiment involving larger values of `n`)\n",
    "    - 6120 students must create a model that works for trigram cases as well\n",
    "    - hint: try to implement the bigram case as a generalized \"n greater than 1\" case\n",
    "- should be *token agnostic* (this means that if we give the model text tokenized as single characters, it will function as a character language model and if we give the model text tokenized as \"words\" (or \"traditionally\"), then it will function as a language model with those tokens)\n",
    "- will use Laplace smoothing\n",
    "- will replace all tokens that occur only once with `<UNK>` at train time\n",
    "    - do not add `<UNK>` to your vocabulary if no tokens in the training data occur only once!\n",
    "\n",
    "We have provided:\n",
    "- a function to read in files\n",
    "- some functions to change a list of strings into tokens\n",
    "- the skeleton of the `LanguageModel` class\n",
    "\n",
    "You need to implement:\n",
    "- all functions marked\n",
    "\n",
    "You may implement:\n",
    "- additional functions/methods as helpful to you\n",
    "\n",
    "As a guideline, including comments, all code required for CS 6120 and some debugging code that can be run with `verbose` parameters, our solution is ~300 lines. (~+120 lines versus the starter code).\n",
    "\n",
    "Points breakdown marked in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename your lm_starter.py file to lm_model.py and put in the same directory as this file\n",
    "import lm_model as lm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the language model (unit tests)\n",
    "import test_minitrainingprovided as test\n",
    "\n",
    "# passing all these tests is a good indication that your model\n",
    "# is correct. They are *not a guarantee*, so make sure to look\n",
    "# at the tests and the cases that they cover. (we'll be testing\n",
    "# your model against all of the testing data in addition).\n",
    "\n",
    "# autograder points in gradescope are assigned SIXTY points\n",
    "# this is essentially 60 points for correctly implementing your\n",
    "# underlying model\n",
    "# there are an additional 10 points manually graded for the correctness\n",
    "# parts of your sentence generation\n",
    "\n",
    "# make sure all training files are in a \"training_files\" directory \n",
    "# that is in the same directory as this notebook\n",
    "\n",
    "unittest = test.TestMiniTraining()\n",
    "unittest.test_createunigrammodellaplace()\n",
    "unittest.test_createbigrammodellaplace()\n",
    "unittest.test_unigramlaplace()\n",
    "unittest.test_unigramunknownslaplace()\n",
    "unittest.test_bigramlaplace()\n",
    "unittest.test_bigramunknownslaplace()\n",
    "# produces output\n",
    "unittest.test_generateunigramconcludes()\n",
    "# produces output\n",
    "unittest.test_generatebigramconcludes()\n",
    "\n",
    "unittest.test_onlyunknownsgenerationandscoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      "Sentence 1: <s> arinell pizza </s>\n",
      "Sentence 2: <s> iranian food anywhere i don't i need to try the fat apple </s>\n",
      "Sentence 3: <s> sujatha's </s>\n",
      "Sentence 4: <s> mediterranean meal ticket </s>\n",
      "Sentence 5: <s> last restaurant a cake </s>\n",
      "Sentence 6: <s> why is i wannu eat pork or friday is actually now so take someone saturday for edy's please would appreciate some place like within say twenty bucks a thousand blocks to change that cost restaurant from the brick hut </s>\n",
      "Sentence 7: <s> are closer </s>\n",
      "Sentence 8: <s> either lunch it's monday evening after two weeks from brazil </s>\n",
      "Sentence 9: <s> regular american or a pretty close inexpensive chinese but um lunch dinner um i'd travel </s>\n",
      "Sentence 10: <s> brazilian brazilian food not thai mexican deli petrouchka </s>\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# instantiate a bigram language model, train it, and generate ten sentences\n",
    "# make sure your output is nicely formatted!\n",
    "ngram = 2\n",
    "training_file_path = \"training_files/berp-training.txt\"\n",
    "# optional parameter tells the tokenize function how to tokenize\n",
    "by_char = False\n",
    "data = lm.read_file(training_file_path)\n",
    "tokens = lm.tokenize(data, ngram, by_char=by_char)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "language_model = lm.LanguageModel(n_gram=2)\n",
    "language_model.train(tokens)\n",
    "generated_sentences = language_model.generate(n=10)\n",
    "print(\"\\nGenerated Sentences:\")\n",
    "for i, sentence in enumerate(generated_sentences, start=1):\n",
    "    print(f\"Sentence {i}: {' '.join(sentence)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of scores: 1.40444635831792e-20\n",
      "The standard deviation of scores is: 1.397406477598118e-19\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "import math\n",
    "# evaluate your bigram model on the test data\n",
    "# score each line in the test data individually, then calculate the average score\n",
    "# you need not re-train your model\n",
    "test_path = \"testing_files/berp-test.txt\"\n",
    "test_data = lm.read_file(test_path)\n",
    "scores = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for i in test_data:\n",
    "    intial_scores=language_model.score(i)\n",
    "    scores.append(intial_scores)\n",
    "\n",
    "\n",
    "# Print out the mean score and standard deviation\n",
    "# for words-as-tokens, these values should be\n",
    "# ~4.9 * 10^-5 and 0.000285\n",
    "\n",
    "mean=sum(scores)/len(scores)\n",
    "\n",
    "squared_diff = [(x - mean) ** 2 for x in scores]\n",
    "\n",
    "# Calculating the variance \n",
    "variance = sum(squared_diff) / len(scores)\n",
    "\n",
    "# Calculating the standard deviation\n",
    "std = math.sqrt(variance)\n",
    "\n",
    "print(f'The mean of scores: {mean}')\n",
    "print(f'The standard deviation of scores is: {std}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Sentences:\n",
      "Sentence 1: <s> who conceives a banker who made ( Pink Floyd . Rutherford 's depths remain visible spectrum are illustrative , Jennifer Love Meter and Logan , autofocus performance style made standardized testing a medium-income country took definitive control and weeks when it stands tall with Buck , augmented with Jason Bonham and LTE service culminated after harvesting . Marshal is revealed his powerful , weight . 59 . Some people even , users in Houston , etc . `` colored people do The prosecution relied too heavily on Jefferson ) and many views of Pigs invasion in poetry is sacrist . 4 to New York Knicks . Marshal Angus Houston , 1951 . There have been toward representative prior to Greater Mexico ; exclusive : Q-W-E-R-T-Y . 6 January 1 Top 17 unanimous winners , given quantity expressed for those of July 1998 ) -3- ( 66 % ) indole ) champion Chicago , Missouri , contain N m ) - and colonial issues have included in business decision , North Carolina , Scott `` Stag ) = 1 , because they held a Puerto Rican singer from adding annatto . October 30 Rockefeller Plaza . 8 December 7 , three times around 8.84 million viewers , imperialism , Uranus had to describe it being sunk . ) Anatomy & E Network of America that shut down to precipitation , High . At elevated levels . methane ) standards for weeks . 4 previous military action , 1982 ) monarchs of Judah c.586 BCE to across Europe since 1960 ) has proved to prominence . 11 July 14 ) to showcase themselves , Stellan Skarsgård and Jewish sect in Virginia ; `` little fluid is being classified in particular subject matter , sex acts of teen sexuality . A banana plugs connected . '' without his dreams , Robby Krieger of difference between duct ) `` encephalomyelitis disseminata '' reimbursement that drink mixed with military leader in marketing messages emphasize a way up democracies , Dominicans experienced mostly small portion of Europe was produced his death on At elevated temperatures in France . There were developed between the stage , Milwaukee Repertory Theater , testing a celebration of sampling material to transport the Moncada Barracks . RAID manufacturers later due primarily to: 0 . 1150-1025 BC , symbolism , Islam and purification can lead in 1959 to China and religious duty in Google Street Band , SmackDown ( Greeks thought . 11 NCAA Tournament games are stanzas in neuroanatomy . 6 pm local youth baseball reserve clause and once again draws fluid previously Kardashian ; Paul Anthelme called junior colleges and mid-1920s , Olivia Newton-John , Texas are able to supply alcohol . to show remained nearly a string of anguilla She was appointed Board , engineers and Robin Yount have at war on the offspring of 1919 to contain claims predominantly accepted by 3rd Wish released or gyproc . Marshal Rooster Cogburn . '' redirect here , mental or legislative branches of 81.8 . to cooperate . RAID 1 A cultural and changes slightly from what time at Wilson Street and state . 30 Species Survival Plans . '' reimbursement that enters the effective operation , 1971 . Film Institute 's constitution after which has more . There are speculated beneath the end . Multiple drafts of southern river valley and NDMP ) zoom range of Dexter had flourished nationwide in planning in over 90 countries that relate , New Japan in wrapping paper Development Index . Multiple sclerosis Treatments attempt to 1951 , color blindness affects more liberal , ranging from meat or burned 86 cars must self assess income increases in 1953 after discovery of Christian theology , unit production manager or convecting ice sheet and slaves were also 40 Albums chart . 58 in historical perspective in brooklyn The discovery , wrote a tenth season after 1988 with braces ( if necessary and objectives for gui The world figure that Aphrodite , below relative to take over two Caribbean Sea and soundtrack of pre-Columbian North and older brother , -inch ( NAALC ) some control . '' Barrett 's town in criminal investigations , D.C . Multiple sclerosis Different structures in 1970 to 32 , the Second City by French and anti-competitive strategies including four-wheel steer or power generation ( 7.4 % in judaism In 1845 astronomers Urbain Le Verrier the titles . Marshals Service `` late 1861 , Jennifer Love On the Christian groups such features of selective restoration activities , voice and storms along with Andrés Iniesta 's interior walls and damages the image elements can not include ground attack came to 2008 and comprise ( Bruno Mars song did mark cuban make interior included misperceptions of controlling crime drama television drama Boardwalk Empire in jaundice ( natural skintone . ' plot follows the walk from Vertically hung drywall with 111 , historians from one by Kumi Koda from Spain 's been part ( 1952 in sales each category are devices together to American western civilization and Young Justice ( where was captured Jerusalem in 1781 . Multiple drafts of 1,500+ independent central government also directly taxable , 1850 compromise of Jesus as Julie Walters was invoked in ten times before taking effect Family reunification ( ' works and hours of experiencing a one-term member from 2000 movie star in Anne Arundel County War —preserving the Non-Aligned Movement during which does erykah badu have three protagonists in research to stop short of debates on location in Ocala , avionics integration , UNESCO in 1998 FIFA , online releases , buffers pH , Greece . Film financing for 3 Apache Software , $ 175.3 million monthly unique visitors more `` # 3 to illness exacerbated by active alliance . , adding annatto . 4 , technical definition section in armed forces were no direct government that one run . 3 Apache Flex applications based largely upon the championship for over time of 19th century drew nearer\n",
      "Sentence 2: <s> how long vilified by wiki 0 , VHS , batteries : gold . Some permit changing , until 1961 . Multiple sclerosis refers to his friendship with Jägermeister logo . miles southwest of being a cheese ; Sesamum . Rutherford introduced many countries of cropper family on Hardangervidda , Jolie made it known species . Billboard Hot regions , Berners-Lee , respectively ; Ulster-Scots : 32.5 in England 0 . Marshal Angus Houston , Minnesota Twins from it larger island until 2007 0 . Some cheeses , Polish , singing about by undetermined means open cockpit , Spanish ) -lipoic acid ( NIT championships , Ohio 0 </s>\n",
      "Sentence 3: <s> When viewed as hematoidin ) award , notably , 10 and 1,282 wounded . This does one get pancreatic cancer The American Olympic medals : Each time employee of unfree labor and children ages near El Mariel in peace and counterbalances that an internationally , around New Horizons spacecraft discovered it became increasingly unpopular , Jennifer Love Hewitt ( T4 ) , gaming devices used on October 30 trail in 1851 . 2 , Akaike information by Me ( such drinks such features and clarinet ( ca Incorporated . ) on cable network hub 0 , Rick Ross , . 3 Consequently , May 5 to stimulate humans’ trichromatic color code to migrate across Japan the compartment or cluster of pancreatic cancer Others have professional minor canons and cloud computing . Marshal Angus Houston , Athenian Empire from what percent from 1663 to Friday , age 36 of boardwalk empire Meanwhile , 1971 with war , resulting leadership style established under the defendant may have occurred . water So there Official Xbox games as 8,000 to Syfy on Jefferson , Wayne stars Matt Groening 's E Network ) who conceives a fellow inmate , then York Governor Eliot Spitzer 's impression of data among multiple presidential republic 's sister 's combination is Ahab . A Christmas and Las Vegas concert broadcast globally via its greatest strength from top marginal tax before 1974 when did fidel castro take place before they have between alpha '' : reliability and surpassing MySpace . Billboard Hot 100 Rhodes Scholars and almost all known life ; 5 turnovers , 1903 . A ) which was published the district is it turned into account of cellular levels of 2.6 million to Win at 15 following table of thin and convicted in Africa and burial site near El Mariel in Berlin Airlift brought embarrassment to decorate with Kurt Angle ( CBS ended in modern transliteration of general , fiddler , Mission Viejo , Tennessee meets in her work occurs from David Hinkley of characters in urban standards because there Official Xbox and business action . Multiple sclerosis In February 1832 , convenience ) prohibits any political agreement signed several navigable rivers extending as 75 % of calcium , resulting careers , Portuguese explorer Vasco Núñez de Balboa crossed the bully Kearney , JMOL , Rick Ross , oxygen systems such as lucrative as well-documented as the basis of such masterpieces as Superintendent of songs . 3 - and vocational skills to area since 2011 amendment to distribute literature . Billboard Hot in high prevalence that included sexual mood of the manga series ended its share of 617 km2 ) at 32.6 . The bomb preparation . ) and written two years lower shipping costs . Known for details of Marcellinus Comes and 1957 , 1734May 10 and bassist Stu Cook , unemployment , meaning is carried out Scott `` Frankly , thus considered a cheese Most denominations , many heroes of instruments , catalog distribution of 1949 , made various lay in modern Natchez as slavery timeline . Marshals are also Rector of Miles Davis , encouraging the surrounding territory and High Times , April 2009 , technology that information of secondary education . ' Bad , people even for Best Actor and storing history was his band The Earth that day it ranked Costa Rican pop group and administered free public speaking as they portrayed Dumbledore for James A. Beaver , respectively ; 13 December 1968 novel `` Somebody to depository institutions throughout much shorter than two or have Her work on September 18 million worth of copper bolts and 1895 and Kylie ) was imminent . There were nationalized , all ports , bassist Stu Cook , Flex , improve dental health . 0 . RAID technology , Little Thing Called Love Hewitt has hurt tourism . 1 Norway 0 </s>\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# see if you can train your model on the data you found for your first homework\n",
    "\n",
    "\n",
    "# what is the maximum value of n <= 10 that you can train a model *in your programming environment* in a reasonable amount of time? (less than 3 - 5 minutes)\n",
    "\n",
    "\n",
    "# generate three sentences with this model\n",
    "\n",
    "ngram = 5\n",
    "training_file_path = \"/Users/harisha/Documents/Northeastern/CS6120/HW1/WikiQA-dev.txt\"\n",
    "# optional parameter tells the tokenize function how to tokenize\n",
    "by_char = False\n",
    "data = lm.read_file(training_file_path)\n",
    "tokens = lm.tokenize(data, ngram, by_char=by_char)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "language_model_HW1 = lm.LanguageModel(n_gram=5)\n",
    "language_model_HW1.train(tokens)\n",
    "generated_sentences = language_model_HW1.generate(n=3)\n",
    "print(\"\\nGenerated Sentences:\")\n",
    "for i, sentence in enumerate(generated_sentences, start=1):\n",
    "    print(f\"Sentence {i}: {' '.join(sentence)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS 6120 REQUIRED\n",
    "----\n",
    "Implement the corresponding function and evaluate the perplexity of your model on the first 20 lines in the test data for values of `n` from 1 to 3. Perplexity should be individually calculated for each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********\n",
      "Ngram model: 1\n",
      "Training completed.\n",
      "Line 1 Perplexity: 162.1347654407583\n",
      "Line 2 Perplexity: 71.22971384987702\n",
      "Line 3 Perplexity: 160.41251701751108\n",
      "Line 4 Perplexity: 97.74075042996172\n",
      "Line 5 Perplexity: 137.24695755220102\n",
      "Line 6 Perplexity: 69.84148819025351\n",
      "Line 7 Perplexity: 106.83857724259228\n",
      "Line 8 Perplexity: 676.0614458215491\n",
      "Line 9 Perplexity: 61.39804372864305\n",
      "Line 10 Perplexity: 29048.500000000007\n",
      "Line 11 Perplexity: 2645.8096045086804\n",
      "Line 12 Perplexity: 89.42355377164832\n",
      "Line 13 Perplexity: 100.45141290964563\n",
      "Line 14 Perplexity: 46.41241510715459\n",
      "Line 15 Perplexity: 178.0760596997564\n",
      "Line 16 Perplexity: 57.97663876004931\n",
      "Line 17 Perplexity: 236.9282399991417\n",
      "Line 18 Perplexity: 70.38521497123516\n",
      "Line 19 Perplexity: 25.870698513586138\n",
      "Line 20 Perplexity: 587.2921726851498\n",
      "********\n",
      "Ngram model: 2\n",
      "Training completed.\n",
      "Line 1 Perplexity: 162.1347654407583\n",
      "Line 2 Perplexity: 71.22971384987702\n",
      "Line 3 Perplexity: 160.41251701751108\n",
      "Line 4 Perplexity: 97.74075042996172\n",
      "Line 5 Perplexity: 137.24695755220102\n",
      "Line 6 Perplexity: 69.84148819025351\n",
      "Line 7 Perplexity: 106.83857724259228\n",
      "Line 8 Perplexity: 676.0614458215491\n",
      "Line 9 Perplexity: 61.39804372864305\n",
      "Line 10 Perplexity: 29048.500000000007\n",
      "Line 11 Perplexity: 2645.8096045086804\n",
      "Line 12 Perplexity: 89.42355377164832\n",
      "Line 13 Perplexity: 100.45141290964563\n",
      "Line 14 Perplexity: 46.41241510715459\n",
      "Line 15 Perplexity: 178.0760596997564\n",
      "Line 16 Perplexity: 57.97663876004931\n",
      "Line 17 Perplexity: 236.9282399991417\n",
      "Line 18 Perplexity: 70.38521497123516\n",
      "Line 19 Perplexity: 25.870698513586138\n",
      "Line 20 Perplexity: 587.2921726851498\n",
      "********\n",
      "Ngram model: 3\n",
      "Training completed.\n",
      "Line 1 Perplexity: 914.026250810712\n",
      "Line 2 Perplexity: 528.2206438099881\n",
      "Line 3 Perplexity: 453.74864896735824\n",
      "Line 4 Perplexity: 346.4510650053161\n",
      "Line 5 Perplexity: 363.3648428369467\n",
      "Line 6 Perplexity: 266.7569586831923\n",
      "Line 7 Perplexity: 433.83741107077526\n",
      "Line 8 Perplexity: 1205.6967751411828\n",
      "Line 9 Perplexity: 171.38891525186176\n",
      "Line 10 Perplexity: 29048.500000000007\n",
      "Line 11 Perplexity: 6898.976056776977\n",
      "Line 12 Perplexity: 379.6381703525499\n",
      "Line 13 Perplexity: 353.9013316198313\n",
      "Line 14 Perplexity: 609.8597694416608\n",
      "Line 15 Perplexity: 763.4025162835201\n",
      "Line 16 Perplexity: 696.9494896961102\n",
      "Line 17 Perplexity: 2623.434767554756\n",
      "Line 18 Perplexity: 782.9577897957288\n",
      "Line 19 Perplexity: 92.77073199242288\n",
      "Line 20 Perplexity: 1193.7220400161973\n"
     ]
    }
   ],
   "source": [
    "test_path = \"testing_files/berp-test.txt\"\n",
    "test_data = lm.read_file('/Users/harisha/Downloads/HW3/testing_files/berp-test.txt')\n",
    "\n",
    "for ngram in range(1, 4):\n",
    "    print(\"********\")\n",
    "    print(\"Ngram model:\", ngram)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    language_model_prex = lm.LanguageModel(n_gram=ngram)\n",
    "    language_model_prex.train(tokens, verbose=True) \n",
    "    \n",
    "    for i, line in enumerate(test_data[:20], start=1):\n",
    "        line_tokens = lm.tokenize_line(line, ngram, by_char=False)\n",
    "        perplexity = language_model.perplexity(line_tokens)\n",
    "        print(f\"Line {i} Perplexity: {perplexity}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the common attributes of the test sentences that cause very high perplexity? \n",
    "\n",
    "Some of the common attributes for high preplexity are:\n",
    "1) Long sentences: These general have a lot of combination of words and sometimes unseen. Hence it may be difficult to keep track of what's going on.\n",
    "2) Gramatical mistakes: This confuses the model as it may think the improper way is the proper way to write that sentence. Hence the quality of data is important. \n",
    "3) Less frequently occuring words: The model may struggle to assign probabilites to less frequently occuring words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 points in this assignment are reserved for overall style (both for writing and for code submitted). All work submitted should be clear, easily interpretable, and checked for spelling, etc. (Re-read what you write and make sure it makes sense). Course staff are always happy to give grammatical help (but we won't pre-grade the content of your answers)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
